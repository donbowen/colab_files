{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbowen/colab_files/blob/main/demos/intervention_demo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w7qX84Ig-K0"
      },
      "source": [
        "# Intervention Demo\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/safety-research/circuit-tracer/blob/main/demos/intervention_demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "In this demo, you'll learn how to perform interventions on models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab\n",
        "!mkdir -p repository && cd repository && \\\n",
        "git clone https://github.com/safety-research/circuit-tracer && \\\n",
        "curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n",
        "uv pip install -e circuit-tracer/"
      ],
      "metadata": {
        "id": "BZKsiibbiGkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  !rm -rf repository"
      ],
      "metadata": {
        "id": "H0PT2ijViyzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from huggingface_hub import notebook_login\n",
        "sys.path.append('repository/circuit-tracer')\n",
        "sys.path.append('repository/circuit-tracer/demos')\n",
        "notebook_login(new_session=False)\n",
        "IN_COLAB = True\n"
      ],
      "metadata": {
        "id": "k5oJEC3xqmOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYqVfbQig-K1"
      },
      "outputs": [],
      "source": [
        "# # #@title Colab Setup Environment\n",
        "\n",
        "# try:\n",
        "#     !rm -rf repository\n",
        "# except:\n",
        "#     pass\n",
        "\n",
        "# try:\n",
        "#     import google.colab\n",
        "#     !mkdir -p repository && cd repository && \\\n",
        "#      git clone https://github.com/safety-research/circuit-tracer && \\\n",
        "#      curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n",
        "#      uv pip install -e circuit-tracer/\n",
        "\n",
        "#     import sys\n",
        "#     from huggingface_hub import notebook_login\n",
        "#     sys.path.append('repository/circuit-tracer')\n",
        "#     sys.path.append('repository/circuit-tracer/demos')\n",
        "#     notebook_login(new_session=False)\n",
        "#     IN_COLAB = True\n",
        "# except ImportError:\n",
        "#     IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v5RGrBAg-K2"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "\n",
        "from circuit_tracer import ReplacementModel\n",
        "\n",
        "# display functions\n",
        "from utils import display_topk_token_predictions, display_generations_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLplsWaWg-K2"
      },
      "source": [
        "First, we load our models (see `attribute_demo.ipynb` for more details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZSld46Ug-K2"
      },
      "outputs": [],
      "source": [
        "model = ReplacementModel.from_pretrained(\"google/gemma-2-2b\", \"gemma\", dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzpyRW7Fg-K2"
      },
      "source": [
        "We'll write some helper functions to print the top next tokens of our model, and a class to store features in."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
        "# max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
        "# desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
        "# max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
        "# batch_size=256  # Batch size when attributing\n",
        "# offload='disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
        "# verbose = True  # Whether to display a tqdm progress bar and timing report\n"
      ],
      "metadata": {
        "id": "rhaNN1IFl6ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from circuit_tracer import ReplacementModel, attribute\n",
        "# from circuit_tracer.utils import create_graph_files\n",
        "\n",
        "# graph = attribute(\n",
        "#     prompt=prompt,\n",
        "#     model=model,\n",
        "#     max_n_logits=max_n_logits,\n",
        "#     desired_logit_prob=desired_logit_prob,\n",
        "#     batch_size=batch_size,\n",
        "#     max_feature_nodes=max_feature_nodes,\n",
        "#     offload=offload,\n",
        "#     verbose=verbose\n",
        "# )\n"
      ],
      "metadata": {
        "id": "nM4HXQgul9Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "\n",
        "# graph_dir = 'graphs'\n",
        "# graph_name = 'example_graph.pt'\n",
        "# graph_dir = Path(graph_dir)\n",
        "# graph_dir.mkdir(exist_ok=True)\n",
        "# graph_path = graph_dir / graph_name\n",
        "\n",
        "# graph.to_pt(graph_path)"
      ],
      "metadata": {
        "id": "Z4aMF4OFl9Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# slug = \"dallas-austin\"  # this is the name that you assign to the graph\n",
        "# graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
        "# node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
        "# edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
        "\n",
        "# create_graph_files(\n",
        "#     graph_or_path=graph_path,  # the graph to create files for\n",
        "#     slug=slug,\n",
        "#     output_path=graph_file_dir,\n",
        "#     node_threshold=node_threshold,\n",
        "#     edge_threshold=edge_threshold\n",
        "# )\n"
      ],
      "metadata": {
        "id": "cSf5cS2hosx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from circuit_tracer.frontend.local_server import serve\n",
        "\n",
        "\n",
        "# port = 8046\n",
        "# server = serve(data_dir='./graph_files/', port=port)\n",
        "\n",
        "# if IN_COLAB:\n",
        "#     from google.colab import output as colab_output  # noqa\n",
        "#     colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)\n",
        "# else:\n",
        "#     from IPython.display import IFrame\n",
        "#     print(f\"Use the IFrame below, or open your graph here: f'http://localhost:{port}/index.html'\")\n",
        "#     display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
      ],
      "metadata": {
        "id": "xqvvbKs6os4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# server.stop()"
      ],
      "metadata": {
        "id": "wwcjPpzIos_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "es1w5-HKl9Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVV28D88l94h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from circuit_tracer.frontend.local_server import serve\n",
        "from circuit_tracer import ReplacementModel, attribute\n",
        "from circuit_tracer.utils import create_graph_files\n",
        "\n",
        "prompt = '''Given the following loan application from 2022:\n",
        "Single-family home\n",
        "30 year fixed-rate mortgage\n",
        "Credit score: 640\n",
        "Loan to value ratio: 75\n",
        "Debt to income ratio: 30\n",
        "Race: White\n",
        "\n",
        "Should this loan be approved? Yes or no answer only please.\n",
        "'''\n",
        "\n",
        "\n",
        "graph_name = 'loanW.pt'\n",
        "slug = \"loanW\"  # this is the name that you assign to the graph\n",
        "\n",
        "graph_dir = 'graphs'\n",
        "graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
        "\n",
        "\n",
        "# Define parameters for attribution\n",
        "# Adjust batch_size to a smaller value to reduce memory usage\n",
        "batch_size = 64  # You might need to experiment with this value based on your GPU memory\n",
        "# Use 'disk' or 'cpu' offload if in Colab or running locally without enough GPU memory\n",
        "offload = 'disk' if IN_COLAB else 'cpu'\n",
        "# You can also add other parameters like max_n_logits and desired_logit_prob if needed\n",
        "max_n_logits = 10\n",
        "desired_logit_prob = 0.95\n",
        "max_feature_nodes = 6000 # Keep this limit to manage memory\n",
        "\n",
        "graph = attribute(\n",
        "    prompt=prompt,\n",
        "    model=model,\n",
        "    batch_size=batch_size, # Use the smaller batch size\n",
        "    offload=offload,     # Use offload\n",
        "    max_n_logits=max_n_logits,\n",
        "    desired_logit_prob=desired_logit_prob,\n",
        "    max_feature_nodes=max_feature_nodes,\n",
        "    verbose=True # Keep verbose to see progress\n",
        ")\n",
        "\n",
        "# after the heavy lifting is done...\n",
        "graph_dir = Path(graph_dir)\n",
        "graph_dir.mkdir(exist_ok=True)\n",
        "graph_path = graph_dir / graph_name\n",
        "graph.to_pt(graph_path)\n",
        "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
        "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
        "\n",
        "create_graph_files(\n",
        "    graph_or_path=graph_path,  # the graph to create files for\n",
        "    slug=slug,\n",
        "    output_path=graph_file_dir,\n",
        "    node_threshold=node_threshold,\n",
        "    edge_threshold=edge_threshold\n",
        ")\n",
        "\n",
        "port = 8046\n",
        "server = serve(data_dir='./graph_files/', port=port)\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import output as colab_output  # noqa\n",
        "    colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)\n",
        "else:\n",
        "    from IPython.display import IFrame\n",
        "    print(f\"Use the IFrame below, or open your graph here: f'http://localhost:{port}/index.html'\")\n",
        "    display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
      ],
      "metadata": {
        "id": "_1KerYE7lxwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mc4BxcZYvHbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)"
      ],
      "metadata": {
        "id": "A1efgaIplxuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVY9doWwlxri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1lw5MeEslxpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6uFg2p_lwmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9baPg_Pg-K3"
      },
      "outputs": [],
      "source": [
        "Feature = namedtuple('Feature', ['layer', 'pos', 'feature_idx'])\n",
        "\n",
        "# a display function that needs the model's tokenizer\n",
        "display_topk_token_predictions = partial(display_topk_token_predictions, tokenizer=model.tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsJTP1P6g-K3"
      },
      "source": [
        "## Example: Changing languages with zero ablations\n",
        "\n",
        "Imagine that you have the following [annotated attribution graph](https://www.neuronpedia.org/gemma-2-2b/graph?slug=gemma-michael-jordan-es&clerps=%5B%5B%222308855%22%2C%22sports%22%5D%2C%5B%222502222%22%2C%22Spanish+articles%22%5D%2C%5B%222513416%22%2C%22Spanish%22%5D%2C%5B%222509334%22%2C%22Spanish%22%5D%2C%5B%222413490%22%2C%22Spanish%22%5D%2C%5B%222403018%22%2C%22Spanish%22%5D%2C%5B%222407980%22%2C%22Spanish+articles%22%5D%2C%5B%222511463%22%2C%22Spanish%22%5D%2C%5B%222104818%22%2C%22basketball%22%5D%2C%5B%222109324%22%2C%22sports%22%5D%2C%5B%222009090%22%2C%22basketball%22%5D%2C%5B%221712431%22%2C%22sports%22%5D%2C%5B%221515208%22%2C%22play%22%5D%2C%5B%22401305%22%2C%22game%22%5D%2C%5B%22109339%22%2C%22a%2Fal+in+Spanish%22%5D%2C%5B%2213978%22%2C%22romance+languages%22%5D%2C%5B%2215822%22%2C%22romance+languages%22%5D%2C%5B%221404939%22%2C%22play%22%5D%2C%5B%221915763%22%2C%22sports%22%5D%2C%5B%221812672%22%2C%22basketball%22%5D%2C%5B%221414510%22%2C%22sports%22%5D%2C%5B%22401742%22%2C%22basketball%22%5D%2C%5B%22101173%22%2C%22basketball%22%5D%2C%5B%22411%22%2C%22famous+people+%2F+named+entities%22%5D%2C%5B%222000341%22%2C%22Spanish%22%5D%2C%5B%222303604%22%2C%22sports+%2F+table+tennis+%2F+pool+%22%5D%2C%5B%222413277%22%2C%22%28incomprehensible%29%22%5D%5D&pinnedIds=27_143831_6%2C25_13416_6%2C24_3018_6%2C25_9334_6%2C24_13490_6%2C25_2222_6%2C24_7980_6%2C25_11463_6%2C21_9324_6%2C21_4818_6%2C23_8855_6%2C20_9090_6%2C17_12431_6%2C15_15208_6%2C14_4939_6%2C4_1305_6%2C1_9339_6%2CE_113501_5%2C0_13978_5%2C0_15822_5%2CE_717_6%2C19_15763_6%2C18_12672_6%2C4_1742_4%2C14_14510_4%2C1_1173_4%2CE_18853_4%2CE_7939_3%2C0_411_4%2C20_341_6&supernodes=%5B%5B%22basketball%22%2C%2220_9090_6%22%2C%2218_12672_6%22%2C%2221_4818_6%22%2C%2223_8855_6%22%5D%2C%5B%22sports%22%2C%2217_12431_6%22%2C%2219_15763_6%22%2C%2221_9324_6%22%5D%2C%5B%22play%22%2C%224_1305_6%22%2C%2214_4939_6%22%2C%2215_15208_6%22%5D%2C%5B%22basketball%22%2C%224_1742_4%22%2C%221_1173_4%22%5D%2C%5B%22romance+language%22%2C%221_9339_6%22%2C%220_15822_5%22%2C%220_13978_5%22%5D%2C%5B%22Spanish%22%2C%2225_9334_6%22%2C%2225_13416_6%22%2C%2224_13490_6%22%2C%2224_7980_6%22%2C%2224_3018_6%22%2C%2225_2222_6%22%2C%2225_11463_6%22%2C%2220_341_6%22%5D%5D&clickedId=20_341_6) showing the circuit for the Spanish sentence *Hecho: Michael Jordan juega al*, or in English, *Fact: Michael Jordan plays*. The correct answer, which the model correctly predicts, is *baloncesto*, or *basketball*. We observe a supernode of features that correspond to the Spanish language. Can we intervene on these features to change the model's output?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/safety-research/circuit-tracer/main/demos/img/gemma/mj-basketball-es.png\" width=\"400\">\n",
        "\n",
        "First, we can try to do this by identifying these supernode features, which we store below. For each, we store their layer, position (here, always -1, as all of these features are active at the final position), and feature ID. For the sake of convenience, we'll only add one supernode feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3SztrSog-K3"
      },
      "outputs": [],
      "source": [
        "supernode_features = [\n",
        "    Feature(layer=20,pos=-1,feature_idx=341),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PwPTfobg-K3"
      },
      "source": [
        "Next, we need to turn our supernode features into a list of intervention tuples. These tuples are formatted as (layer, node, feature_idx, new_feature_value). For now, let's try just zeroing out these features at the last position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivh-iJNwg-K3"
      },
      "outputs": [],
      "source": [
        "intervention_tuples = [(*supernode_feature, 0.0) for supernode_feature in supernode_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr2-JOaeg-K3"
      },
      "source": [
        "Finally, we can run the intervention and view its effects on the model's output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PWwzsZCg-K4"
      },
      "outputs": [],
      "source": [
        "s = \"The sky is blue, so today is \"\n",
        "\n",
        "with torch.inference_mode():\n",
        "    original_logits = model(s)\n",
        "    new_logits, _ = model.feature_intervention(s, intervention_tuples)\n",
        "\n",
        "display_topk_token_predictions(s, original_logits, new_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLTisFByg-K4"
      },
      "source": [
        "That wasn't very effective! We do see that the probability of *basketball* has risen, bringing it into the top 5. But intervening on just one feature isn't enough to change the model's behavior dramatically; the rest of the distribution remains more or less the same. This is because many Spanish features contribute to our model's output language, while we have changed only one. Try changing more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01JnRmbJg-K4"
      },
      "source": [
        "## Example: Swapping languages by turning features on\n",
        "\n",
        "In the last example, we only turned Spanish features off, yielding text in English, which seems to be the model's default. But what if we wanted to swap to another language?\n",
        "Then we'd have to turn language features from that language on. Let's try this with another language, French. Here is the attribution graph for the analogous French sentence, [`Fait: Michael Jordan joue au` →`basket`](https://www.neuronpedia.org/gemma-2-2b/graph?slug=gemma-basket&clickedId=17_10566_2&clerps=%5B%5B%222308855%22%2C%22basketball%22%5D%2C%5B%222502222%22%2C%22Spanish+articles%22%5D%2C%5B%222513416%22%2C%22Spanish%22%5D%2C%5B%222104818%22%2C%22basketball%22%5D%2C%5B%222109324%22%2C%22sports%22%5D%2C%5B%222009090%22%2C%22basketball%22%5D%2C%5B%221712431%22%2C%22sports%22%5D%2C%5B%221515208%22%2C%22play%22%5D%2C%5B%22401305%22%2C%22game%22%5D%2C%5B%2213978%22%2C%22romance+languages%22%5D%2C%5B%2215822%22%2C%22romance+languages%22%5D%2C%5B%221404939%22%2C%22play%22%5D%2C%5B%221915763%22%2C%22sports%22%5D%2C%5B%221812672%22%2C%22basketball%22%5D%2C%5B%221414510%22%2C%22sports%22%5D%2C%5B%22401742%22%2C%22basketball%22%5D%2C%5B%22101173%22%2C%22basketball%22%5D%2C%5B%22411%22%2C%22famous+people+%2F+named+entities%22%5D%2C%5B%221710566%22%2C%22French%22%5D%5D&pinnedIds=27_12220_7%2CE_18853_5%2C21_4818_7%2C21_9324_7%2C23_3604_7%2C25_14882_7%2C24_15306_7%2C23_15317_7%2C20_9090_7%2C24_3329_7%2C19_15763_7%2C18_12672_7%2C17_12431_7%2C17_5253_7%2C15_15208_7%2C14_4939_7%2C6_7377_7%2CE_78224_6%2C4_1305_7%2C3_305_7%2C24_2086_7%2C24_3772_7%2C21_16354_7%2C20_1454_7%2C23_2592_7%2C22_10566_7%2C23_2554_7%2C17_10566_6%2C0_4076_6%2C14_14575_6%2C7_11689_6%2C4_1742_5%2C1_1173_5%2CE_7939_4&supernodes=%5B%5B%22game%2Fplay%22%2C%223_305_7%22%2C%224_1305_7%22%2C%226_7377_7%22%2C%2215_15208_7%22%2C%2214_4939_7%22%5D%2C%5B%22French%22%2C%220_4076_6%22%2C%227_11689_6%22%2C%2214_14575_6%22%2C%2217_10566_6%22%5D%2C%5B%22basketball%22%2C%2221_4818_7%22%2C%2218_12672_7%22%5D%2C%5B%22sports%22%2C%2217_12431_7%22%2C%2217_5253_7%22%2C%2221_9324_7%22%2C%2220_9090_7%22%2C%2219_15763_7%22%2C%2223_3604_7%22%2C%2223_15317_7%22%5D%2C%5B%22basketball%22%2C%224_1742_5%22%2C%221_1173_5%22%5D%2C%5B%22French%22%2C%2224_3329_7%22%2C%2221_16354_7%22%2C%2220_1454_7%22%2C%2223_2592_7%22%2C%2223_2554_7%22%2C%2224_2086_7%22%2C%2224_15306_7%22%2C%2225_14882_7%22%2C%2224_3772_7%22%2C%2222_10566_7%22%5D%5D).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/safety-research/circuit-tracer/main/demos/img/gemma/mj-basketball-fr.png\" width=\"400\">\n",
        "\n",
        "The answer to the French query is \"basket\". Can we change that to Spanish? We start by taking one relatively low-level French feature, that feeds into all of the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh7EcApIg-K4"
      },
      "outputs": [],
      "source": [
        "french_supernode_features = [Feature(layer=20,pos=-1,feature_idx=1454)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0RfW_olg-K4"
      },
      "source": [
        "But what should we set the values of the French supernode features to be? Ideally, we set them to some in-distribution values. To do this, we can get the activations of these nodes on the French input sentence. We'll get these as a sparse tensor, to save on memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOLPEt3wg-K4"
      },
      "outputs": [],
      "source": [
        "s_spanish = \"Hecho: Michael Jordan juega al\"\n",
        "_, activations = model.get_activations(s_spanish, sparse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4P3LPJLg-K4"
      },
      "source": [
        "Now, we construct and perform the intervention! Each supernode_feature contains precisely the information needed to index into `activations`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85wS5W_dg-K4"
      },
      "outputs": [],
      "source": [
        "spanish_supernode_features = supernode_features  # from before\n",
        "fr_es_intervention_tuples = [(*supernode_feature, 0.0) for supernode_feature in french_supernode_features]\n",
        "fr_es_intervention_tuples+= [(*supernode_feature, 10*activations[supernode_feature]) for (supernode_feature) in spanish_supernode_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkpxjKjlg-K4"
      },
      "outputs": [],
      "source": [
        "s_french = \"Fait: Michael Jordan joue au\"\n",
        "\n",
        "with torch.inference_mode():\n",
        "    original_logits = model(s_french)\n",
        "    new_logits, _ = model.feature_intervention(s_french, fr_es_intervention_tuples)\n",
        "\n",
        "display_topk_token_predictions(s_french, original_logits, new_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5phmQvxg-K5"
      },
      "source": [
        "## Example: Interventions + Sampling\n",
        "We've now intervened twice on the last token of the sentence; interventions on other positions work analogously. But what if we want to intervene in an open-ended fashion, allowing our model to generate tokens with that intervention still active? We can do this as follows, by setting the position of our intervention to an open-ended slice: `slice(pos, None, None)`. We set `pos` to be the last token of the original input, but you can also set it to an earlier position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQzBjxoAg-K5"
      },
      "outputs": [],
      "source": [
        "sequence_length = len(model.tokenizer(s_spanish).input_ids)\n",
        "original_feature_pos = sequence_length - 1\n",
        "open_ended_slice = slice(original_feature_pos, None, None)\n",
        "open_ended_es_fr_intervention_tuples = [(layer, open_ended_slice, feature_idx, 0.0) for (layer, _, feature_idx) in spanish_supernode_features]\n",
        "open_ended_es_fr_intervention_tuples+= [(layer, open_ended_slice, feature_idx, 10*activations[layer, orig_pos, feature_idx]) for (layer, orig_pos, feature_idx) in spanish_supernode_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUxC2_OQg-K5"
      },
      "source": [
        "Now, we just get the necessary hooks by calling `model._get_feature_intervention_hooks`, and use those hooks to generate! Make sure to set `use_past_kv_cache` to false, otherwise the model will attempt to generate using the KV cache + length=1 inputs; this is more efficient, but makes interventions hard. `do_sample` is off here for consistency, but you can turn it on as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VMdqAOwg-K5"
      },
      "outputs": [],
      "source": [
        "hooks, _ = model._get_feature_intervention_hooks(s_french, open_ended_es_fr_intervention_tuples, freeze_attention=False)\n",
        "pre_intervention_generation = [model.generate(s_french, do_sample=False, use_past_kv_cache=False, verbose=False)]\n",
        "with model.hooks(hooks):\n",
        "    post_intervention_generation = [model.generate(s_french, do_sample=False, use_past_kv_cache=False, verbose=False)]\n",
        "\n",
        "display_generations_comparison(s_french, pre_intervention_generation, post_intervention_generation)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}